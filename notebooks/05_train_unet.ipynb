{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de233e0-a3de-472b-b3a4-3939836dc170",
   "metadata": {},
   "source": [
    "# Train U-Net on multiclass problems\n",
    "\n",
    "This is very similar code to `11_unet_carvana.ipynb` but we have 2 classes instead of 1.\n",
    "\n",
    "With approximately 400 images in the training set, I trained for 100 epochs and get very good results for face tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aafef6-1f53-47b4-96e3-8af83fc1cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import albumentations as A\n",
    "import cv2\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.unet import Unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca89a04-3bec-4342-9100-0576ee8a3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'normalization_values': {'means': [0.5110162496566772, 0.4608974754810333, 0.4772901237010956], 'stds': [0.2727729380130768, 0.2578601539134979, 0.256255567073822]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (128.0, 0.0, 128.0)], 'objects': ['nose', 'chin', 'rEye', 'lEye'], 'target_radius': 10}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf836a27-2efd-4e4f-b117-c7055cec81fd",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7b231b-ee84-4204-afb5-4f486a164beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-4\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")) \n",
    "BATCH_SIZE=4\n",
    "NUM_EPOCHS = 10\n",
    "NUM_WORKERS = 4\n",
    "OUTPUT_CHANNELS = len(project.object_list)\n",
    "IMAGE_HEIGHT = project.image_size[0]\n",
    "IMAGE_WIDTH = project.image_size[1]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "TRAIN_IMG_DIR =\"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_images\"\n",
    "TRAIN_MASK_DIR =\"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_masks\" \n",
    "TRAIN_COORDINATE_DIR =\"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_coordinates\" \n",
    "VAL_IMG_DIR = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_images\"\n",
    "VAL_MASK_DIR =\"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_masks\"\n",
    "VAL_COORDINATE_DIR =\"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_coordinates\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a08fc-1cde-466b-9917-086be4b5cf94",
   "metadata": {},
   "source": [
    "## Model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7729f9-1692-4a26-97f6-083b8853d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=OUTPUT_CHANNELS).to(DEVICE)\n",
    "if LOAD_MODEL:\n",
    "    project.load_model(model)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # not doing sigmoid on the output of the model, so use this, if we had more classes (objects) we would use change out_chan and cross_entropy_loss as loss_fn\n",
    "optimizer= optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ef1c3-3370-4a29-b25f-3b17398bb494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data augmentation and normalization\n",
    "\n",
    "I am using the [albumentations](https://albumentations.ai/) package to do data augmentation.\n",
    "\n",
    "We might want to do some data augmentation when training so that the images are modified slightly between epochs. This improves generalization of the model and prevent overfitting.\n",
    "\n",
    "We also want to perform data normalization so that the mean of each channel is 0 and the std is 1. This facilitate learning. See the notebook on data normalization.\n",
    "\n",
    "Here I am using 4 transformations. We can set the probability that this transformation is applied using the `p` argument. You can set it in the project configuration file. Alternatively, you can edit the code below.\n",
    "\n",
    "Tips\n",
    "\n",
    "* If you are tracking left/right body parts, you probably don't want to flip your images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c27d33e-2ef9-4e2d-ae0e-8aceb41df302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose([\n",
      "  RandomSizedCrop(always_apply=False, p=1.0, min_max_height=(430, 480), height=480, width=640, w2h_ratio=1.3333333333333333, interpolation=1),\n",
      "  HorizontalFlip(always_apply=False, p=0.0),\n",
      "  Rotate(always_apply=False, p=0.3, limit=(-30, 30), interpolation=1, border_mode=0, value=None, mask_value=None, rotate_method='largest_box', crop_border=False),\n",
      "  RandomBrightnessContrast(always_apply=False, p=0.2, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.5110162496566772, 0.4608974754810333, 0.4772901237010956], std=[0.2727729380130768, 0.2578601539134979, 0.256255567073822], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n",
      "Compose([\n",
      "  Normalize(always_apply=False, p=1.0, mean=[0.5110162496566772, 0.4608974754810333, 0.4772901237010956], std=[0.2727729380130768, 0.2578601539134979, 0.256255567073822], max_pixel_value=255.0),\n",
      "], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={})\n"
     ]
    }
   ],
   "source": [
    "original_height = project.image_size[0]\n",
    "original_width = project.image_size[1]\n",
    "means = project.normalization_values[\"means\"]\n",
    "stds = project.normalization_values[\"stds\"]\n",
    "\n",
    "\n",
    "trainTransform = A.Compose([   \n",
    "                    A.RandomSizedCrop(min_max_height=(original_height-50, original_height),w2h_ratio=original_width/original_height,height=original_height, width=original_width, p=project.augmentation_RandomSizedCropProb),\n",
    "                    A.HorizontalFlip(p=project.augmentation_HorizontalFlipProb),\n",
    "                    A.Rotate (limit=30,border_mode=cv2.BORDER_CONSTANT,p=project.augmentation_RotateProb),\n",
    "                    A.RandomBrightnessContrast(p=project.augmentation_RandomBrightnessContrastProb),\n",
    "                    A.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "valTransform = A.Compose([   \n",
    "                    A.Normalize(mean=means, std=stds)\n",
    "])\n",
    "\n",
    "\n",
    "print(trainTransform)\n",
    "print(valTransform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512b108-1935-45d9-9e9a-de939e710d13",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfa5b17-653a-4db8-9c31-d5c7e72176ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = UNetDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR,TRAIN_COORDINATE_DIR, transform=trainTransform)\n",
    "valDataset = UNetDataset(VAL_IMG_DIR, VAL_MASK_DIR,VAL_COORDINATE_DIR, transform=valTransform)\n",
    "trainLoader = DataLoader(trainDataset,shuffle=True,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY)\n",
    "valLoader = DataLoader(valDataset,shuffle=False,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory = PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab3a05a-01e1-4a9c-b4cb-bb266f1a0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=2\n",
    "trainLoader = DataLoader(trainDataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)\n",
    "valLoader = DataLoader(valDataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b334993f-4f13-4c09-a056-66955f3b2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 480, 640]), torch.Size([2, 4, 480, 640]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, masks, _ = next(iter(trainLoader))\n",
    "imgs.shape, masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b3647-f375-4080-8c9f-0d508bd2ac67",
   "metadata": {},
   "source": [
    "There is a lot of black because half of our pixels are below 0, on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea71b15-19ff-4fe3-b6ef-a0afd0f15e13",
   "metadata": {},
   "source": [
    "# Save and load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0991b991-75e3-497d-ac6e-d2b5fb91fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    #print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b0c4c-b328-40bc-a6a3-c09489dd7200",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2bbbef-a284-4fa6-89ad-906bc5712ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader,model,device=\"cuda\"):\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y, _ in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            preds = torch.sigmoid(model(x))\n",
    "            preds = (preds > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2*(preds * y).sum() / ((preds+y).sum() + 1e-8)) # work only for binary\n",
    "            \n",
    "            # proportion of the mask detected\n",
    "            num_mask = y.sum()\n",
    "            num_mask_detected = preds[y==1.0].sum()\n",
    "            num_detected = preds.sum()\n",
    "                        \n",
    "    print(f\"Accuracy: {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader):.2f}\")\n",
    "    print(f\"Mask pixels detected: {num_mask_detected/num_mask*100:.2f}%\")\n",
    "    print(f\"False positives: {(num_detected-num_mask_detected)/num_detected*100:.2f}%\")\n",
    "    model.train()     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b011fe-fd1f-45d4-9c20-cfe078631b01",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438e23e0-a918-4aa4-82a3-04d71cfbf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader,model,optimizer,loss_fn,scaler,epoch,total_epochs):\n",
    "    \"\"\"\n",
    "    One epoch of training\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data,targets,_) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "            \n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # update tqdm loop\n",
    "        loop.set_postfix_str(\"loss: {:.7f}, epoch: {:d}/{:d}\".format(loss.item(),epoch,total_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03760e0-5a98-4d58-be63-d4e98dc3c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 2022-11-26 14:19:31.146527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 232/232 [00:30<00:00,  7.60it/s, loss: 0.0005777, epoch: 0/10]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.98\n",
      "Dice score: 0.89\n",
      "Mask pixels detected: 92.17%\n",
      "False positives: 11.04%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 232/232 [00:29<00:00,  7.78it/s, loss: 0.0007374, epoch: 1/10]\n",
      "100%|███████████| 232/232 [00:29<00:00,  7.80it/s, loss: 0.0006644, epoch: 2/10]\n",
      " 34%|████        | 78/232 [00:09<00:20,  7.66it/s, loss: 0.0004306, epoch: 3/10]"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "print(\"Starting time:\",startTime)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_fn(trainLoader,model,optimizer,loss_fn,scaler,epoch,NUM_EPOCHS)\n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint,filename=os.path.join(project.models_dir,\"my_checkpoint.pth.tar\"))\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(valLoader,model,device=DEVICE)\n",
    "\n",
    "endTime=datetime.now()\n",
    "print(\"End time:\",endTime)\n",
    "print(\"{} epochs, duration:\".format(NUM_EPOCHS), endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7e64d-8b80-42e3-ad71-cea889a30e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623ea2ca-3e67-4937-8eca-fa68558001e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
