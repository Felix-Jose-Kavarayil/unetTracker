{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d3c990-021e-4f9a-a7a1-abfaa7f2900b",
   "metadata": {},
   "source": [
    "# Track videos with your trained network\n",
    "\n",
    "Once you have a well-trained network, you can use it to extract the position of the different objects in a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0865ea4b-ca58-4053-81f9-0916232a926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.unet import Unet\n",
    "from unetTracker.utils import extract_object_position_from_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "784bd8c6-8a03-4fb6-abcc-73e38192e50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'normalization_values': {'means': [0.5110162496566772, 0.4608974754810333, 0.4772901237010956], 'stds': [0.2727729380130768, 0.2578601539134979, 0.256255567073822]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (128.0, 0.0, 128.0)], 'objects': ['nose', 'chin', 'rEye', 'lEye'], 'target_radius': 10}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")\n",
    "dataset = UNetDataset(image_dir=project.image_dir, mask_dir=project.mask_dir, coordinate_dir=project.coordinate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d8f39f9-69a0-478d-8734-351a5d806779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\"))\n",
    "print(\"Training on device {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccc6eaa7-f829-4e27-bd91-502f949c0bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3,out_channels=len(project.object_list)).to(device)\n",
    "fn=project.model_fn\n",
    "model.load_state_dict(torch.load(fn,map_location=device))\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b55e2038-4e5c-4b01-8b42-8f6e11bb2c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading valTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/valTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"valTransform\")\n",
    "print(\"Loading valTransform from\", fileName)\n",
    "valTransform=pickle.load(open(fileName, \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c630d4bb-60df-407e-8c69-1ddaff214bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fn = '/tmp/video1.avi'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e422aef-d2df-412c-a169-f145d477e079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frames: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██████████▉                                | 76/300 [00:04<00:11, 20.03it/s]"
     ]
    }
   ],
   "source": [
    "df = extract_object_position_from_video(project=project,transform=valTransform,\n",
    "                                        model=model,device=device,\n",
    "                                        video_fn=video_fn,blobMinArea=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305e31c-cea2-4baa-984b-ca150ca7f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "newFileName = (os.path.splitext(video_fn)[0]) + \".csv\"\n",
    "print(\"Saving \",newFileName)\n",
    "df.to_csv(newFileName,index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d647fd-87b5-43eb-b025-1e05b508ced5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(newFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6232e2-05b6-408c-a6ee-f8a7ad39eaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in project.object_list:\n",
    "    plt.plot(df[f\"{obj}_x\"],df[f\"{obj}_y\"],label=obj)\n",
    "plt.xlim(0,640)\n",
    "plt.ylim(0,480)\n",
    "plt.ylim(max(plt.ylim()), min(plt.ylim()))\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
