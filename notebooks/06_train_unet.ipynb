{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de233e0-a3de-472b-b3a4-3939836dc170",
   "metadata": {},
   "source": [
    "# Train U-Net\n",
    "\n",
    "This is very similar code to `11_unet_carvana.ipynb` but we have 2 classes instead of 1.\n",
    "\n",
    "With approximately 400 images in the training set, I trained for 100 epochs and get very good results for face tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aafef6-1f53-47b4-96e3-8af83fc1cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.unet import Unet\n",
    "from unetTracker.coordinatesFromSegmentationMask import CoordinatesFromSegmentationMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca89a04-3bec-4342-9100-0576ee8a3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'normalization_values': {'means': [0.5110162496566772, 0.4608974754810333, 0.4772901237010956], 'stds': [0.2727729380130768, 0.2578601539134979, 0.256255567073822]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (128.0, 0.0, 128.0)], 'objects': ['nose', 'chin', 'rEye', 'lEye'], 'target_radius': 10}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf836a27-2efd-4e4f-b117-c7055cec81fd",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b7b231b-ee84-4204-afb5-4f486a164beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-4\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")) \n",
    "BATCH_SIZE=3\n",
    "NUM_EPOCHS = 20\n",
    "NUM_WORKERS = 4\n",
    "OUTPUT_CHANNELS = len(project.object_list)\n",
    "IMAGE_HEIGHT = project.image_size[0]\n",
    "IMAGE_WIDTH = project.image_size[1]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "TRAIN_IMAGE_DIR = os.path.join(project.dataset_dir,\"train_images\")\n",
    "TRAIN_MASK_DIR =  os.path.join(project.dataset_dir,\"train_masks\")\n",
    "TRAIN_COORDINATE_DIR = os.path.join(project.dataset_dir,\"train_coordinates\")\n",
    "VAL_IMAGE_DIR = os.path.join(project.dataset_dir,\"val_images\")\n",
    "VAL_MASK_DIR =  os.path.join(project.dataset_dir,\"val_masks\")\n",
    "VAL_COORDINATE_DIR = os.path.join(project.dataset_dir,\"val_coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a08fc-1cde-466b-9917-086be4b5cf94",
   "metadata": {},
   "source": [
    "## Model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7729f9-1692-4a26-97f6-083b8853d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=OUTPUT_CHANNELS).to(DEVICE)\n",
    "if LOAD_MODEL:\n",
    "    project.load_model(model)\n",
    "\n",
    "# set the model in train mode\n",
    "model.train()\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # not doing sigmoid on the output of the model, so use this, if we had more classes (objects) we would use change out_chan and cross_entropy_loss as loss_fn\n",
    "optimizer= optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ef1c3-3370-4a29-b25f-3b17398bb494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data augmentation and normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c27d33e-2ef9-4e2d-ae0e-8aceb41df302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/trainTransform\n",
      "Loading valTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/valTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"trainTransform\")\n",
    "print(\"Loading trainTransform from\", fileName)\n",
    "trainTransform=pickle.load(open(fileName,\"rb\" ))\n",
    "\n",
    "fileName = os.path.join(project.augmentation_dir,\"valTransform\")\n",
    "print(\"Loading valTransform from\", fileName)\n",
    "valTransform=pickle.load(open(fileName, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512b108-1935-45d9-9e9a-de939e710d13",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfa5b17-653a-4db8-9c31-d5c7e72176ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = UNetDataset(TRAIN_IMAGE_DIR, TRAIN_MASK_DIR,TRAIN_COORDINATE_DIR, transform=trainTransform)\n",
    "valDataset = UNetDataset(VAL_IMAGE_DIR, VAL_MASK_DIR,VAL_COORDINATE_DIR, transform=valTransform)\n",
    "trainLoader = DataLoader(trainDataset,shuffle=True,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY)\n",
    "valLoader = DataLoader(valDataset,shuffle=False,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory = PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab3a05a-01e1-4a9c-b4cb-bb266f1a0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainLoader = DataLoader(trainDataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)\n",
    "valLoader = DataLoader(valDataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b334993f-4f13-4c09-a056-66955f3b2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 480, 640]), torch.Size([2, 4, 480, 640]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, masks, _ = next(iter(trainLoader))\n",
    "imgs.shape, masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b3647-f375-4080-8c9f-0d508bd2ac67",
   "metadata": {},
   "source": [
    "There is a lot of black because half of our pixels are below 0, on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea71b15-19ff-4fe3-b6ef-a0afd0f15e13",
   "metadata": {},
   "source": [
    "# Save and load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0991b991-75e3-497d-ac6e-d2b5fb91fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    #print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b0c4c-b328-40bc-a6a3-c09489dd7200",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b2bbbef-a284-4fa6-89ad-906bc5712ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model,loader,device):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    num_mask = 0\n",
    "    num_mask_detected = 0\n",
    "    num_detected = 0\n",
    "    sum_distance = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y,c in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            output = torch.sigmoid(model(x))\n",
    "            preds = (output > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2*(preds * y).sum() / ((preds+y).sum() + 1e-8)) # work only for binary\n",
    "\n",
    "            # proportion of the mask detected\n",
    "            num_mask += y.sum()\n",
    "            num_mask_detected += preds[y==1.0].sum()\n",
    "            num_detected += preds.sum()\n",
    "\n",
    "            # distance between predicted coordinates and labelled coordinates\n",
    "            output = output.detach().cpu().numpy()\n",
    "            pred_coords = cDetector.detect(output)\n",
    "\n",
    "            sum_distance+= np.nanmean(np.sqrt(((pred_coords[:,:,0:2] - c.numpy())**2).sum(axis=2)))\n",
    "            # we acutally do a mean of the error for the different objects in a batch\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader):.2f}\")\n",
    "    print(f\"Mask pixels detected: {num_mask_detected/num_mask*100:.2f}%\")\n",
    "    print(f\"False positives: {(num_detected-num_mask_detected)/num_detected*100:.2f}%\")\n",
    "    print(f\"Mean distance: {sum_distance/len(loader)}\")\n",
    "    a = model.train()\n",
    "\n",
    "cDetector = CoordinatesFromSegmentationMask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b011fe-fd1f-45d4-9c20-cfe078631b01",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "438e23e0-a918-4aa4-82a3-04d71cfbf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader,model,optimizer,loss_fn,scaler,epoch,total_epochs):\n",
    "    \"\"\"\n",
    "    One epoch of training\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data,targets,_) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "            \n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # update tqdm loop\n",
    "        loop.set_postfix_str(\"loss: {:.7f}, epoch: {:d}/{:d}\".format(loss.item(),epoch,total_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b03760e0-5a98-4d58-be63-d4e98dc3c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 2022-12-10 17:54:20.617617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:41<00:00,  7.38it/s, loss: 0.0078137, epoch: 0/20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.96\n",
      "Dice score: 0.76\n",
      "Mask pixels detected: 67.04%\n",
      "False positives: 10.91%\n",
      "Mean distance: 3.9035888416248254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:39<00:00,  7.79it/s, loss: 0.0004424, epoch: 1/20]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:39<00:00,  7.82it/s, loss: 0.0007015, epoch: 2/20]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:40<00:00,  7.70it/s, loss: 0.0006260, epoch: 3/20]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:40<00:00,  7.56it/s, loss: 0.0004213, epoch: 4/20]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 309/309 [00:40<00:00,  7.61it/s, loss: 0.0038322, epoch: 5/20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.97\n",
      "Dice score: 0.82\n",
      "Mask pixels detected: 81.75%\n",
      "False positives: 16.81%\n",
      "Mean distance: 4.168949637131351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|██████████████████████████████████████████▍                                                         | 131/309 [00:17<00:23,  7.59it/s, loss: 0.0004542, epoch: 6/20]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting time:\u001b[39m\u001b[38;5;124m\"\u001b[39m,startTime)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[0;32m----> 5\u001b[0m     \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainLoader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;66;03m# save model\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         checkpoint \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mstate_dict()}\n",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(loader, model, optimizer, loss_fn, scaler, epoch, total_epochs)\u001b[0m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     18\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 19\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# update tqdm loop\u001b[39;00m\n",
      "File \u001b[0;32m~/torch3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:341\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_(optimizer)\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 341\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_opt_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    343\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mSTEPPED\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/torch3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:287\u001b[0m, in \u001b[0;36mGradScaler._maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    286\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf_per_device\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    288\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "File \u001b[0;32m~/torch3/lib/python3.8/site-packages/torch/cuda/amp/grad_scaler.py:287\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_maybe_opt_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, optimizer, optimizer_state, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    286\u001b[0m     retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    288\u001b[0m         retval \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mstep(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retval\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "print(\"Starting time:\",startTime)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_fn(trainLoader,model,optimizer,loss_fn,scaler,epoch,NUM_EPOCHS)\n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint,filename=os.path.join(project.models_dir,\"my_checkpoint.pth.tar\"))\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(model,valLoader,DEVICE)\n",
    "\n",
    "endTime=datetime.now()\n",
    "print(\"End time:\",endTime)\n",
    "print(\"{} epochs, duration:\".format(NUM_EPOCHS), endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcc7e64d-8b80-42e3-ad71-cea889a30e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model state dict to /home/kevin/Documents/trackingProjects/faceTrack/models/UNet.pt\n",
      "2022-12-10 17:58:56.490580\n"
     ]
    }
   ],
   "source": [
    "project.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d4be8-a5c4-4d45-b236-5ce0bdb8033c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
