{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de233e0-a3de-472b-b3a4-3939836dc170",
   "metadata": {},
   "source": [
    "# Train U-Net\n",
    "\n",
    "This is very similar code to `11_unet_carvana.ipynb` but we have 2 classes instead of 1.\n",
    "\n",
    "With approximately 400 images in the training set, I trained for 100 epochs and get very good results for face tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25aafef6-1f53-47b4-96e3-8af83fc1cf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.dataset import UNetDataset\n",
    "from unetTracker.unet import Unet\n",
    "from unetTracker.coordinatesFromSegmentationMask import CoordinatesFromSegmentationMask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca89a04-3bec-4342-9100-0576ee8a3a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'normalization_values': {'means': [0.5110162496566772, 0.4608974754810333, 0.4772901237010956], 'stds': [0.2727729380130768, 0.2578601539134979, 0.256255567073822]}, 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (128.0, 0.0, 128.0)], 'objects': ['nose', 'chin', 'rEye', 'lEye'], 'target_radius': 10}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf836a27-2efd-4e4f-b117-c7055cec81fd",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b7b231b-ee84-4204-afb5-4f486a164beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE=1e-4\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")) \n",
    "BATCH_SIZE=2\n",
    "NUM_EPOCHS = 40\n",
    "NUM_WORKERS = 4\n",
    "OUTPUT_CHANNELS = len(project.object_list)\n",
    "IMAGE_HEIGHT = project.image_size[0]\n",
    "IMAGE_WIDTH = project.image_size[1]\n",
    "PIN_MEMORY = True\n",
    "LOAD_MODEL = True\n",
    "TRAIN_IMAGE_DIR = os.path.join(project.dataset_dir,\"train_images\")\n",
    "TRAIN_MASK_DIR =  os.path.join(project.dataset_dir,\"train_masks\")\n",
    "TRAIN_COORDINATE_DIR = os.path.join(project.dataset_dir,\"train_coordinates\")\n",
    "VAL_IMAGE_DIR = os.path.join(project.dataset_dir,\"val_images\")\n",
    "VAL_MASK_DIR =  os.path.join(project.dataset_dir,\"val_masks\")\n",
    "VAL_COORDINATE_DIR = os.path.join(project.dataset_dir,\"val_coordinates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7a08fc-1cde-466b-9917-086be4b5cf94",
   "metadata": {},
   "source": [
    "## Model, loss, and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c7729f9-1692-4a26-97f6-083b8853d615",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Unet(in_channels=3, out_channels=OUTPUT_CHANNELS).to(DEVICE)\n",
    "if LOAD_MODEL:\n",
    "    project.load_model(model)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss() # not doing sigmoid on the output of the model, so use this, if we had more classes (objects) we would use change out_chan and cross_entropy_loss as loss_fn\n",
    "optimizer= optim.Adam(model.parameters(),lr=LEARNING_RATE)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1ef1c3-3370-4a29-b25f-3b17398bb494",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data augmentation and normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c27d33e-2ef9-4e2d-ae0e-8aceb41df302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading trainTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/trainTransform\n",
      "Loading valTransform from /home/kevin/Documents/trackingProjects/faceTrack/augmentation/valTransform\n"
     ]
    }
   ],
   "source": [
    "fileName = os.path.join(project.augmentation_dir,\"trainTransform\")\n",
    "print(\"Loading trainTransform from\", fileName)\n",
    "trainTransform=pickle.load(open(fileName,\"rb\" ))\n",
    "\n",
    "fileName = os.path.join(project.augmentation_dir,\"valTransform\")\n",
    "print(\"Loading valTransform from\", fileName)\n",
    "valTransform=pickle.load(open(fileName, \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d512b108-1935-45d9-9e9a-de939e710d13",
   "metadata": {},
   "source": [
    "## Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bfa5b17-653a-4db8-9c31-d5c7e72176ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = UNetDataset(TRAIN_IMAGE_DIR, TRAIN_MASK_DIR,TRAIN_COORDINATE_DIR, transform=trainTransform)\n",
    "valDataset = UNetDataset(VAL_IMAGE_DIR, VAL_MASK_DIR,VAL_COORDINATE_DIR, transform=valTransform)\n",
    "trainLoader = DataLoader(trainDataset,shuffle=True,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory=PIN_MEMORY)\n",
    "valLoader = DataLoader(valDataset,shuffle=False,batch_size=BATCH_SIZE, num_workers=NUM_WORKERS,pin_memory = PIN_MEMORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ab3a05a-01e1-4a9c-b4cb-bb266f1a0106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trainLoader = DataLoader(trainDataset,\n",
    "                          shuffle=True,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)\n",
    "valLoader = DataLoader(valDataset,\n",
    "                          shuffle=False,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b334993f-4f13-4c09-a056-66955f3b2e15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 480, 640]), torch.Size([2, 4, 480, 640]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs, masks, _ = next(iter(trainLoader))\n",
    "imgs.shape, masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02b3647-f375-4080-8c9f-0d508bd2ac67",
   "metadata": {},
   "source": [
    "There is a lot of black because half of our pixels are below 0, on average.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea71b15-19ff-4fe3-b6ef-a0afd0f15e13",
   "metadata": {},
   "source": [
    "# Save and load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0991b991-75e3-497d-ac6e-d2b5fb91fec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename = \"my_checkpoint.pth.tar\"):\n",
    "    #print(\"Saving checkpoint\")\n",
    "    torch.save(state,filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b0c4c-b328-40bc-a6a3-c09489dd7200",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b2bbbef-a284-4fa6-89ad-906bc5712ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(model,loader,device):\n",
    "\n",
    "    num_correct = 0\n",
    "    num_pixels = 0\n",
    "    dice_score = 0\n",
    "    num_mask = 0\n",
    "    num_mask_detected = 0\n",
    "    num_detected = 0\n",
    "    sum_distance = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x,y,c in loader:\n",
    "            x = x.to(DEVICE)\n",
    "            y = y.to(DEVICE)\n",
    "            output = torch.sigmoid(model(x))\n",
    "            preds = (output > 0.5).float()\n",
    "            num_correct += (preds == y).sum()\n",
    "            num_pixels += torch.numel(preds)\n",
    "            dice_score += (2*(preds * y).sum() / ((preds+y).sum() + 1e-8)) # work only for binary\n",
    "\n",
    "            # proportion of the mask detected\n",
    "            num_mask += y.sum()\n",
    "            num_mask_detected += preds[y==1.0].sum()\n",
    "            num_detected += preds.sum()\n",
    "\n",
    "            # distance between predicted coordinates and labelled coordinates\n",
    "            output = output.detach().cpu().numpy()\n",
    "            pred_coords = cDetector.detect(output)\n",
    "\n",
    "            sum_distance+= np.nanmean(np.sqrt(((pred_coords[:,:,0:2] - c.numpy())**2).sum(axis=2)))\n",
    "            # we acutally do a mean of the error for the different objects in a batch\n",
    "\n",
    "\n",
    "    print(f\"Accuracy: {num_correct/num_pixels*100:.2f}\")\n",
    "    print(f\"Dice score: {dice_score/len(loader):.2f}\")\n",
    "    print(f\"Mask pixels detected: {num_mask_detected/num_mask*100:.2f}%\")\n",
    "    print(f\"False positives: {(num_detected-num_mask_detected)/num_detected*100:.2f}%\")\n",
    "    print(f\"Mean distance: {sum_distance/len(loader)}\")\n",
    "    a = model.train()\n",
    "\n",
    "cDetector = CoordinatesFromSegmentationMask()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b011fe-fd1f-45d4-9c20-cfe078631b01",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "438e23e0-a918-4aa4-82a3-04d71cfbf1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(loader,model,optimizer,loss_fn,scaler,epoch,total_epochs):\n",
    "    \"\"\"\n",
    "    One epoch of training\n",
    "    \"\"\"\n",
    "    loop = tqdm(loader)\n",
    "    for batch_idx, (data,targets,_) in enumerate(loop):\n",
    "        data = data.to(device=DEVICE)\n",
    "        targets = targets.to(device=DEVICE)\n",
    "        \n",
    "        # forward\n",
    "        with torch.cuda.amp.autocast():\n",
    "            predictions = model(data)\n",
    "            loss = loss_fn(predictions,targets)\n",
    "            \n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # update tqdm loop\n",
    "        loop.set_postfix_str(\"loss: {:.7f}, epoch: {:d}/{:d}\".format(loss.item(),epoch,total_epochs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03760e0-5a98-4d58-be63-d4e98dc3c5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting time: 2022-11-30 12:29:35.831944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 274/274 [00:35<00:00,  7.66it/s, loss: 0.0003778, epoch: 0/40]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.97\n",
      "Dice score: 0.83\n",
      "Mask pixels detected: 86.80%\n",
      "False positives: 19.82%\n",
      "Mean distance: 5.035939312040807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████| 274/274 [00:35<00:00,  7.77it/s, loss: 0.0084718, epoch: 1/40]\n",
      "100%|███████████| 274/274 [00:34<00:00,  7.95it/s, loss: 0.0003939, epoch: 2/40]\n",
      "100%|███████████| 274/274 [00:34<00:00,  7.98it/s, loss: 0.0002810, epoch: 3/40]\n",
      "100%|███████████| 274/274 [00:34<00:00,  7.98it/s, loss: 0.0043276, epoch: 4/40]\n",
      "100%|███████████| 274/274 [00:34<00:00,  8.01it/s, loss: 0.0005546, epoch: 5/40]\n"
     ]
    }
   ],
   "source": [
    "startTime = datetime.now()\n",
    "print(\"Starting time:\",startTime)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    train_fn(trainLoader,model,optimizer,loss_fn,scaler,epoch,NUM_EPOCHS)\n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        # save model\n",
    "        checkpoint = {\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()}\n",
    "        save_checkpoint(checkpoint,filename=os.path.join(project.models_dir,\"my_checkpoint.pth.tar\"))\n",
    "\n",
    "        # check accuracy\n",
    "        check_accuracy(model,valLoader,DEVICE)\n",
    "\n",
    "endTime=datetime.now()\n",
    "print(\"End time:\",endTime)\n",
    "print(\"{} epochs, duration:\".format(NUM_EPOCHS), endTime-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7e64d-8b80-42e3-ad71-cea889a30e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "project.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5d4be8-a5c4-4d45-b236-5ce0bdb8033c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
