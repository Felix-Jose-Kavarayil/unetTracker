{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset for image segmentation from a live video camera\n",
    "\n",
    "We now use a simple GUI to generate new dataset for image segmentation. \n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "\n",
    "* Get an image from a camera (could later be a video or images). \n",
    "* Click on the video to extract one image, and then click again to label the oject on the image.\n",
    "* Save image, coordinate and mask to the dataset.\n",
    "* Split images and mask as a train and validation dataset.\n",
    "\n",
    "You want at least 150 imgaes to get ok tracking. We can further improve the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from ipyevents import Event \n",
    "import threading\n",
    "from IPython.display import display\n",
    "\n",
    "from unetTracker.trackingProject import TrackingProject\n",
    "from unetTracker.multiClassUNetDataset import MultiClassUNetDataset\n",
    "from unetTracker.camera import USBCamera, bgr8_to_jpeg\n",
    "from unetTracker.unetGUI import LabelFromCameraGUI\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a TrackingProject object from file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project directory: /home/kevin/Documents/trackingProjects/faceTrack\n",
      "Loading /home/kevin/Documents/trackingProjects/faceTrack/config.yalm\n",
      "{'augmentation_HorizontalFlipProb': 0.0, 'augmentation_RandomBrightnessContrastProb': 0.2, 'augmentation_RandomSizedCropProb': 1.0, 'augmentation_RotateProb': 0.3, 'image_size': [480, 640], 'labeling_ImageEnlargeFactor': 2.0, 'name': 'faceTrack', 'object_colors': [(0.0, 0.0, 255.0), (255.0, 0.0, 0.0), (255.0, 255.0, 0.0), (128.0, 0.0, 128.0)], 'objects': ['nose', 'chin', 'rEye', 'lEye'], 'target_radius': 10}\n"
     ]
    }
   ],
   "source": [
    "project = TrackingProject(name=\"faceTrack\",root_folder = \"/home/kevin/Documents/trackingProjects/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset object\n",
    "\n",
    "A Dataset object is created to save labeled images, masks and coordinates. It is also used to load the data into memory when training your model. This is a class inherited from `torch.utils.data.Dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MultiClassUNetDataset(image_dir=project.image_dir, mask_dir=project.mask_dir, coordinates_dir=project.coordinates_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can know how many images are in your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "470"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load any of the images in your dataset, together with mask and coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "image,mask,coordinates = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we are getting an image, mask, and coordinates of objects.\n",
    "\n",
    "The image and mask are `torch.tensor` instead of `np.array`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crw-rw----+ 1 root video 81, 1 Nov 25 09:04 /dev/video1\n",
      "crw-rw----+ 1 root video 81, 0 Nov 25 09:04 /dev/video0\n"
     ]
    }
   ],
   "source": [
    "!ls -ltrh /dev/video*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@407.447] global /io/opencv/modules/videoio/src/cap_v4l.cpp (902) open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not initialize camera.  Please see error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/repo/unetTracker/unetTracker/camera.py:59\u001b[0m, in \u001b[0;36mUSBCamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m re:\n\u001b[0;32m---> 59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not read image from camera.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m camera \u001b[38;5;241m=\u001b[39m \u001b[43mUSBCamera\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/unetTracker/unetTracker/camera.py:62\u001b[0m, in \u001b[0;36mUSBCamera.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not read image from camera.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not initialize camera.  Please see error trace.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     65\u001b[0m atexit\u001b[38;5;241m.\u001b[39mregister(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcap\u001b[38;5;241m.\u001b[39mrelease)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not initialize camera.  Please see error trace."
     ]
    }
   ],
   "source": [
    "camera = USBCamera(width=project.image_size[1], height=project.image_size[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "image = camera.read()\n",
    "print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0318d316045e4eada87a13d6f8a6dbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00C\\x00\\x02\\x01\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgWidget = widgets.Image(format='jpeg')\n",
    "imgWidget.value = bgr8_to_jpeg(image)\n",
    "display(imgWidget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the GUI to extract and label frames from a camera \n",
    "\n",
    "1. Click on the top left image to capture a frame from the camera feed. A larger image will appear below the entry boxes for the coordinates. \n",
    "2. Your body parts are listed above the large image. In the large image, click on the object that is selected in the radio button. You should see a label apper in the picture at the very bottom.\n",
    "3. Repeat for all the body parts visible on the image. If you leave the coordinate of an object at 0,0, it is considered not in the image.\n",
    "4. Once all the body parts are labeled, click on `Save labelled frame`.\n",
    "5. Go back to step 1 and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c62e12c6328645d69966b1345b50cd64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='240.0', width='320.0'), Image(value=b'',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gui = LabelFromCameraGUI(camera,project,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gui.stop_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you click on `Save labelled frame`, a new data point is added to your dataset. This is stored in a subdirectory of your project directory called `dataset`. There are 3 directories there: images, masks and coordinates. This data will be used to train the network and assess tracking quality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training and validation sets\n",
    "\n",
    "The images and masks were saved in an `images` and `masks` directories. We need to create `train_images`, `train_masks`, `val_images` and `val_masks` directories.\n",
    "\n",
    "We will use the validation folders to estimate the accuracy of our model on data that it has not seen during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/kevin/Documents/trackingProjects/faceTrack/dataset/images',\n",
       " '/home/kevin/Documents/trackingProjects/faceTrack/dataset/mask')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project.image_dir,project.mask_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 277\n",
      "Length of validation set: 49\n",
      "Copying files to training and validation directories\n"
     ]
    }
   ],
   "source": [
    "dataset.create_training_validation_dataset(train_images_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_images\",\n",
    "                                           train_masks_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_masks\",\n",
    "                                           train_coordinates_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_coordinates\",\n",
    "                                               \n",
    "                                           val_images_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_images\",\n",
    "                                           val_masks_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_masks\",\n",
    "                                           val_coordinates_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_coordinates\",\n",
    "                                           test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "277"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_images\"\n",
    "train_masks_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_masks\"\n",
    "train_coordinates_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/train_coordinates\"\n",
    "trainDataset = MultiClassUNetDataset(image_dir=train_images_dir, mask_dir=train_masks_dir,coordinates_dir=train_coordinates_dir)\n",
    "len(trainDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_images_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_images\"\n",
    "val_masks_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_masks\"\n",
    "val_coordinates_dir = \"/home/kevin/Documents/trackingProjects/faceTrack/dataset/val_coordinates\"\n",
    "valDataset = MultiClassUNetDataset(image_dir=val_images_dir, mask_dir=val_masks_dir,coordinates_dir=val_coordinates_dir)\n",
    "len(valDataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train a network with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
